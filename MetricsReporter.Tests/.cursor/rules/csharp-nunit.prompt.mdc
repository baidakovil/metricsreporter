---
globs: ["**/*.cs", "**/*.csproj"]
description: 'Best practices for NUnit unit testing'
alwaysApply: true
---

# NUnit Best Practices for Revit Add-ins

Your goal is to help me write effective unit tests with NUnit for a Revit add-in project, covering both standard and data-driven testing approaches with consideration for Revit API context.

## Project Setup

- Use separate test projects for every csproj projects with naming convention `[ProjectName].Tests`
- Target framework should match main project: `net8.0-windows` for Revit compatibility
- Required package references (pinned for reproducibility):
  - `Microsoft.NET.Test.Sdk` version `17.9.0`
  - `NUnit` version `3.14.0`
  - `NUnit3TestAdapter` version `4.5.0`
  - `FluentAssertions` version `6.12.0`
  - `NSubstitute` version `5.1.0`
- Create test classes that match the classes being tested (e.g., `PythonExecutionServiceTests` for `PythonExecutionService`)
- Use .NET SDK test commands: `dotnet test` for running tests
- Consider that tests in the project [Rca.Integration.Revit.Tests](./../../Rca.Integration.Revit.Tests) are not unit tests. These tests uses custom test adapters Rca.TestAdapter for Named Pipe test execution within Revit API context

## Test Structure

- Apply `[TestFixture]` attribute to test classes
- Use `[Test]` attribute for test methods
- Follow the Arrange-Act-Assert (AAA) pattern
- Precede each test with a 1â€“4 line description of the intent: what behavior/scenario it verifies and why it matters
- Name tests using the pattern `MethodName_Scenario_ExpectedBehavior`
- Use `[SetUp]` and `[TearDown]` for per-test setup and teardown
- Use `[OneTimeSetUp]` and `[OneTimeTearDown]` for per-class setup and teardown
- Use `[SetUpFixture]` for assembly-level setup and teardown

## Revit API Context Considerations

- Mock Revit API dependencies using interfaces (e.g., `IPythonExecutionService`)
- Avoid direct Revit API calls in unit tests - use integration tests for those
- Test business logic separately from Revit API interactions
- Use `[Category("Unit")]` for pure unit tests and `[Category("Integration")]` for Revit-dependent tests
- Consider using test doubles for `UIApplication`, `Document`, and other Revit types

## Standard Tests

- Keep tests focused on a single behavior
- Avoid testing multiple behaviors in one test method
- Use clear assertions that express intent with FluentAssertions: `result.Should().Be(expected)`
- Include only the assertions needed to verify the test case
- Make tests independent and idempotent (can run in any order)
- Avoid test interdependencies

## Data-Driven Tests

- Use `[TestCase]` for inline test data
- Use `[TestCaseSource]` for programmatically generated test data
- Use `[Values]` for simple parameter combinations
- Use `[ValueSource]` for property or method-based data sources
- Use `[Random]` for random numeric test values
- Use `[Range]` for sequential numeric test values
- Use `[Combinatorial]` or `[Pairwise]` for combining multiple parameters

## Assertions

- Prefer FluentAssertions for readability: `result.Should().NotBeNull()` over `Assert.IsNotNull(result)`
- Use `Assert.That` with constraint model when FluentAssertions isn't suitable
- Use constraints like `Is.EqualTo`, `Is.SameAs`, `Contains.Item`
- Use `CollectionAssert` for collection comparisons when not using FluentAssertions
- Use `StringAssert` for string-specific assertions when not using FluentAssertions
- Use `Assert.ThrowsAsync<T>` for testing exceptions in async methods
- Use descriptive messages in assertions for clarity on failure

## Async Testing

- Use `async Task` for async test methods (not `async void`)
- Use `Assert.ThrowsAsync<T>` for testing async exceptions
- Consider using `ConfigureAwait(false)` in test code when running under a synchronization context; with modern NUnit and `dotnet test` it is optional
- Test both successful and failed async operations

## Mocking and Isolation

- Use NSubstitute for mocking dependencies: `Substitute.For<IInterface>()`
- Mock all external dependencies including Revit API objects
- Use interfaces to facilitate mocking (follow dependency injection patterns)
- Verify interactions when testing side effects: `mock.Received(1).Method()`
- Use `Returns()` for setting up mock return values
- Use `Throws()` for setting up mock exceptions

## Test Organization

- Group tests by feature or component using nested classes or separate files
- Use categories with `[Category("Unit")]`, `[Category("Integration")]`, `[Category("Revit")]`
- Use `[Order]` to control test execution order when necessary (avoid when possible)
- Use `[Description]` to provide additional test information
- Consider `[Explicit]` for tests that shouldn't run automatically (e.g., slow integration tests)
- Use `[Ignore("Reason")]` to temporarily skip tests with clear reasoning

## Example Test Structure

```csharp
[TestFixture]
public class PythonExecutionServiceTests
{
    private IPythonExecutionService? pythonService;
    
    [SetUp]
    public void SetUp()
    {
        pythonService = Substitute.For<IPythonExecutionService>();
    }
    
    [Test, Category("Unit")]
    public async Task ExecuteAsync_ValidCode_ReturnsExpectedResult()
    {
        // Arrange
        var code = "print('Hello World')";
        var expectedResult = "Hello World";
        pythonService!.ExecuteAsync(code).Returns(Task.FromResult(expectedResult));
        
        // Act
        var result = await pythonService.ExecuteAsync(code).ConfigureAwait(false);
        
        // Assert
        result.Should().Contain("Hello World");
        await pythonService.Received(1).ExecuteAsync(code);
    }
}
